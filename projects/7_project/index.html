<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Echocardiogram Segmentation | Hunter Ma 马思博 </title> <meta name="author" content="Hunter Ma 马思博"> <meta name="description" content="Robotics and Control Lab, UBC"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?6b3e8b623f6fc35ceaddc317f362c92a"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hunterm321.github.io/projects/7_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hunter</span> Ma 马思博 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Echocardiogram Segmentation</h1> <p class="post-description">Robotics and Control Lab, UBC</p> </header> <article> <h4 id="overview">Overview</h4> <p>Throughout the 4 months I stayed in UBC’s <a href="https://rcl.ece.ubc.ca" rel="external nofollow noopener" target="_blank">Robotics and Control Lab</a> (RCL) as a research intern, I worked on image and video segmentation of <a href="https://www.mayoclinic.org/tests-procedures/echocardiogram/about/pac-20393856" rel="external nofollow noopener" target="_blank">echocardiogram</a>. Specifically, I focused on chamber segmentation. The ultimate goal of this project is to teach a single agent to <strong>simultaneously</strong> segment all 4 chambers, which could be done by incorporating semantic information, instead of training individual agent for each chamber segmentation. I mainly worked on two sub-projects. The first one is baseline testing for <a href="https://www.nature.com/articles/s41467-024-44824-z" rel="external nofollow noopener" target="_blank">MedSAM</a> and <a href="https://ai.meta.com/sam2/" rel="external nofollow noopener" target="_blank">SAM 2</a> and fine-tuning <a href="https://huggingface.co/microsoft/Florence-2-large" rel="external nofollow noopener" target="_blank">Florence 2</a>. The second one is visual object tracking and segmentation using <a href="https://yangchris11.github.io/samurai/" rel="external nofollow noopener" target="_blank">SAMURAI</a>, which is also a derivative of SAM 2, as well as fine-tuning SAMURAI for echocardiogram video segmentation. All fine-tunings were done using <a href="https://arxiv.org/pdf/2106.09685" rel="external nofollow noopener" target="_blank">Low-Rank Adaptation</a> (LoRA). I would like to focus more on discussing the SAMURAI project as I think it is the more interesting one.</p> <h4 id="what-is-samurai">What is SAMURAI</h4> <p>SAMURAI stands for <em>SAM-based Unified and Robust zero-shot visual tracker with motion-Aware Instance-level memory</em>. Specifically, SAMURAI is a derivative of the <strong>video object segmentation</strong> (VOS) functionality introduced in SAM 2, seeking to improve the <strong>visual object tracking</strong> (VOT) performance without a huge architectural modification. Maybe before we introduce SAMURAI’s novelty in this domain of knowledge, a quick background of SAM 1 and SAM 2’s architectures would come in handy.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sam2_architecture-480.webp 480w,/assets/img/sam2_architecture-800.webp 800w,/assets/img/sam2_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sam2_architecture.jpg" class="img-fluid rounded" width="100%" height="auto" title="SAM 2 architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> SAM 2 architecture, all modules related to memory are new compared to the original SAM 1 </div> <p>Back when Meta introduced SAM 1, it quickey gained popularity in the image segmentation domain due to its robust performance. SAM 1 receives an image which goes through an <strong>image encoder</strong>, and a prompt which goes through a <strong>prompt encoder</strong>. Additionally, the image and prompt <strong>embeddings</strong> both go through a <strong>mask decoder</strong> which performs <strong>cross-attention</strong> between the prompt and image, as well as <strong>self-attention</strong> of the prompts, to finally output the predicted masks. However, SAM 1 lacks the ability to perform VOS since it does not have the ability to encode any memory information. This is where SAM 2 came in to fill in this gap. Specifically, SAM 2 has the same image encoder, prompt encoder, and mask decoder found in SAM 1, it also adds the ability to use previous mask outputs as reference to aid the current prediction. It does so by the following 3 additions:</p> <ul> <li> <strong>Memory attention</strong>: takes the current frame embedding as input and performs cross-attention between the current frame embedding and previous embeddings from past frames using several <strong>transformer</strong> blocks.</li> <li> <strong>Memory encoder</strong>: encodes the predicted masks of the current frame as embeddings using CNN and saves it as a memory.</li> <li> <strong>Memory bank</strong>: retains fixed-size memories of the past predictions in a First-In-First-Out format which will be used to aid the current mask prediction.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/samurai_architecture-480.webp 480w,/assets/img/samurai_architecture-800.webp 800w,/assets/img/samurai_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/samurai_architecture.png" class="img-fluid rounded" width="100%" height="auto" title="SAMURAI architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> SAMURAI architecture proposes a motion modeling module and a motion-aware memory selection </div> <p>So how does SAMURAI improve upon the model proposed in SAM 2? In short, since SAM 2 already has very robust segmentation performance, SAMURAI focuses on improving the <strong>tracking</strong> quality of the segmented object. It does so by introducing two novelties—<strong>neither</strong> of which requires any additional learnable parameters. The first contribution is the addition of a <strong>motion modeling module</strong>. This module trakcs and predicts the bounding box surrounding the segmented object; specifically, it tracks and predicts the bounding box’s top left coordinate, height and width, as well as their corresponding rate of change (i.e., speed). Using this motion cue, the motion modeling module is able to handle difficult scenarios, such as self-occlusions and fast-moving objects. The second contribution is the addition of <strong>motion-aware memory selection</strong> for memory attention. The new proposed scheme selects high-quality memories from the memory bank. instead of selecting the latest memories using a fix window design. The higher quality memories will directly have an influence during cross-attention with the current frame.</p> <div class="row"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0X1E67F4F64EDDA6ED_out-480.webp 480w,/assets/img/0X1E67F4F64EDDA6ED_out-800.webp 800w,/assets/img/0X1E67F4F64EDDA6ED_out-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/0X1E67F4F64EDDA6ED_out.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sample output 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0X59E3868FDA8BC941_out-480.webp 480w,/assets/img/0X59E3868FDA8BC941_out-800.webp 800w,/assets/img/0X59E3868FDA8BC941_out-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/0X59E3868FDA8BC941_out.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sample output 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0XFA2F04D15D5D222_out-480.webp 480w,/assets/img/0XFA2F04D15D5D222_out-800.webp 800w,/assets/img/0XFA2F04D15D5D222_out-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/0XFA2F04D15D5D222_out.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sample output 3" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Frame extractions of video segmention of LV chamber using SAMURAI on EchoNet-Dynamic dataset </div> <h5 id="motion-modeling">Motion modeling</h5> <p>The addition of the motion modeling module gives SAM 2 another reference to look at when selecting the best mask from all its predictions. The original SAM 2 only generates an affinity score \(s_{\text{aff}}\) and an object score \(s_{\text{obj}}\) for each predicted mask, the best mask is selected by choosing the highest \(s_{\text{aff}}\) such that the corresponding \(s_{\text{obj}}\) is positive:</p> \[\mathbb{M} = \{M_0, M_1, \dots, M_{N-1}\}\] \[M_i = \underset{M_i \in \mathbb{M}}{\mathrm{argmax}} \ s_{\text{aff}}(M_i) \quad \text{where} \quad s_{\text{obj}}(M_i) &gt; 0\] <p>SAMURAI generates another score called the <strong>motion</strong> score \(s_{\text{kf}}\). This score is calculated based on the <strong>IoU</strong> score of the predicted bounding box and each mask generated from SAM 2. In the end, a weighted sum of \(s_{\text{aff}}\) and \(s_{\text{kf}}\) is used to determine the mask with the best score:</p> \[M_i = \underset{M_i \in \mathbb{M}}{\mathrm{argmax}} \ \alpha_{\text{aff}} \cdot s_{\text{aff}}(M_i) + (1 - \alpha_{\text{aff}}) \cdot s_{\text{kf}}(M_i) \quad \text{where} \quad s_{\text{obj}}(M_i) &gt; 0\] <p>You might be interested in why the newly introduced motion tracking score \(s_{\text{kf}}\) bears the subscript <em>kf</em>, that’s because SAMURAI tracks and predicts the bounding box of the segmented object using <strong><a href="https://en.wikipedia.org/wiki/Kalman_filter" rel="external nofollow noopener" target="_blank">Kalman Filter</a></strong> (KF). Kalman Filter has no learnable parameters and was originally introduced in control theory to estimate the <strong>state</strong> of a dynamical system. We can always write down the state of a dynamical system in either <strong>continuous-time ordinary differential equation</strong> (ODE) or <strong>discrete-time algebraic equation</strong> format:</p> \[\dot{\mathbf{x}} = A \mathbf{x} \quad \text{for continuous time}\] \[\mathbf{x}_{t+1} = F \mathbf{x}_{t} \quad \text{for discrete time}\] <p>For obvious reasons our computers always prefer to work with discrete-time systems. I went in-depth into KF in my <a href="../2_project">CubeSat Control System</a> page, so please check it out if you are interested in learning more about it. Here I will go through some of the essential pieces of KF that are unique to SAMURAI instead.</p> <p>As we mentioned earlier, the motion modeling module outputs an estimated position and speed of the bounding box, which we call the <strong>state</strong> of the system. We can encode this state into a vector format:</p> \[\mathbf{x} = \begin{bmatrix} x &amp; y &amp; w &amp; h &amp; \dot{x} &amp; \dot{y} &amp; \dot{w} &amp; \dot{h} \end{bmatrix}^\top\] <p>KF needs to know the dynamics of the system, or in other words, how the system propagates with time. To achieve this, we need the <strong>linear system transition matrix</strong>, or \(F\), in the discrete-time equation above. Note that we consider this to be a simple linear system, otherwise we would need to utilize the Extended Kalman Filter (EKF), which will be a lot more work. Assuming all velocities are constant and the period between two consecutive frames is \(\Delta t\), and using the x-coordinate as an example:</p> \[\begin{aligned} x_{t+1} &amp;= x_t + \dot{x}_t \Delta t \\ \dot{x}_{t+1} &amp;= \dot{x}_t \end{aligned}\] <p>The rest coordinates all follow, and we can wrtie down \(F\):</p> \[F = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \Delta t &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \Delta t &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \Delta t &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \Delta t \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\] <p>The final corrected prediction of the bounding box can be computed as follow:</p> \[\mathbf{x}_{t+1} = \hat{\mathbf{x}}_{t+1} + \mathbf{K}_{t+1} (\mathbf{y}_{k+1} - \mathbf{H} \hat{\mathbf{x}}_{k+1})\] <p>Where:</p> <ul> <li>\(\mathbf{x}_{k+1}\): Final corrected bounding box prediction</li> <li>\(\hat{\mathbf{x}}_{k+1}\): Uncorrected bounding box prediction from the discrete-time linear system equation above</li> <li>\(\mathbf{K}_{k+1}\): Kalman gain</li> <li>\(\mathbf{y}_{k+1}\): Measurement, derived from the selected mask from SAM 2</li> <li>\(\mathbf{H}\): Observation model matrix</li> </ul> <p>Again, all the derivation of these matrices and vectors can be found in my <a href="../2_project">CubeSat Control System</a> page, so take a look if you are interested in knowing the math and theory behind. One main advantage of using Kalman Filter to assist in SAM 2’s VOS, apart from no additional training parameters and ease of use, is the ability to make modifications to the linear system. In SAMURAI, the authors chose to track the eight elements presented above in the state of the dynamical system, but one can easily adjust the state vector \(\mathbf{x}\) to incorporate different information. For example, we may find tracking the top left and bottom right corners of the bounding box more effective than the top left corner plus its height and width. Basically, as long as we can come up with a correct representation of the proposed dynamical system, we can track the bounding box however we desire.</p> <h5 id="motion-aware-memory-selection">Motion-aware memory selection</h5> <p>In the original SAM 2, the preparation of memory for attention is done through selecting the previous \(N\) frames. Specifically, SAM 2 simply selects the most recent \(N\) frames as masks to feed into the memory attention module. This approach does not care about the quality of the memory selection. In SAMURAI, the authors addressed this issue by assigning scores to each mask at every frame and intelligently selecting the previous \(N\) masks whose scores surpass the corresponding threshold.</p> \[B_t = \{ m_i \mid s_{\text{mask}} &gt; \tau_{\text{mask}}, \: s_{\text{obj}} &gt; \tau_{\text{obj}}, \: s_{\text{kf}} &gt; \tau_{\text{kf}}, \: t - N_{\text{max}} \leq i &lt; t \}\] <p>Similar to motion modeling, each mask is assigned an affinity score \(s_{\text{mask}}\), an object occurrence score \(s_{\text{obj}}\), and a motion score \(s_{\text{kf}}\). SAMURAI has a maximum number of frames \(N_{\text{max}}\) to look back, for all frames within this period, only \(N\) frames are selected if their masks’ \(s_{\text{mask}}\), \(s_{\text{obj}}\), and \(s_{\text{kf}}\) pass the predetermined thresholds \(\tau_{\text{mask}}\), \(\tau_{\text{obj}}\), and \(\tau_{\text{kf}}\). This appraoch encompasses motion cue into memory selection, hence significantly enhancing the VOT performance of SAM 2.</p> <h5 id="fine-tuning">Fine-tuning</h5> <p>The addition of motion modeling and motion-aware memory selection already proves SAMURAI can greatly increase the performance of SAM 2’s VOS. However, SAMURAI still uses the pre-trained weights from SAM 2 and SAM 2 was built for a more general task. SAMURAI, or the concept behind SAMURAI, has the ability to be virtually plugged into any other vision language models. At the time of this project, <a href="https://supermedintel.github.io/Medical-SAM2/" rel="external nofollow noopener" target="_blank">Medical SAM 2</a> (MedSAM 2), which is built on top of SAM 2 specifically for segmenting medical images and also supports VOS, was not open-sourced yet. This made fine-tuning SAMURAI our only option, and also a useful learning journey. We opted for LoRA for fine-tuning due to its significant reduction of trainable parameters while retaining effective tuning results. Here is a short summary of how LoRA works:</p> <h6 id="1-full-weight-update">1. Full Weight Update</h6> <p>In standard fine-tuning, a weight matrix \(\mathbf{W} \in \mathbb{R}^{d \times k}\) is updated by learning an additive term \(\Delta \mathbf{W}\):</p> \[\mathbf{W}' = \mathbf{W} + \Delta \mathbf{W}\] <h6 id="2-low-rank-factorization">2. Low-Rank Factorization</h6> <p>LoRA replaces the large \(\Delta \mathbf{W}\) with a factorized version \(\mathbf{A}\mathbf{B}\), where:</p> \[\mathbf{A} \in \mathbb{R}^{d \times r}, \quad \mathbf{B} \in \mathbb{R}^{r \times k}, \quad r \ll \min(d,k)\] <p>Thus,</p> \[\Delta \mathbf{W} = \mathbf{A}\mathbf{B}\] <p>This drastically reduces the number of trainable parameters.</p> <h6 id="3-lora-update-rule">3. LoRA Update Rule</h6> <p>The new weight matrix is:</p> \[\mathbf{W}' = \mathbf{W} + \alpha \mathbf{A}\mathbf{B}\] <p>where \(\alpha\) is a scalar that scales the low-rank update.</p> <ul> <li> <strong>Original weights \(\mathbf{W}\)</strong> remain frozen.</li> <li>Only the low-rank factors \(\mathbf{A}\) and \(\mathbf{B}\) are trained.</li> </ul> <h6 id="4-parameter-efficiency">4. Parameter Efficiency</h6> <ul> <li> <strong>Full Fine-Tuning:</strong> \(\Delta \mathbf{W}\) has \(d \times k\) parameters.</li> <li> <strong>LoRA:</strong> \(\mathbf{A}\) and \(\mathbf{B}\) have \(d \times r + r \times k\) parameters, with \(r\) being far smaller than \(d\) or \(k\).</li> </ul> <p>This substantially lowers memory and computational cost.</p> <h6 id="5-why-it-works">5. Why It Works</h6> <ul> <li> <strong>Base Model Knowledge:</strong> The original \(\mathbf{W}\) is already well-trained.</li> <li> <strong>Lightweight Adaptation:</strong> A low-rank update \(\mathbf{AB}\) is enough to tailor the model to new tasks, serving as a regularized and efficient update.</li> </ul> <p>Acccording to LoRA’s paper, the intermediate rank \(r\) can be as low as 1 or 2 and the result would be still on par with if not outperforming the full rank of \(\Delta \mathbf{W}\) when fine-tuning large language models. In our application, we specifically targeted important layers, namely the mask decoder and memory attention layers, and froze the other layers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lora_config</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="c1"># Mask decoder layers
</span>        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.self_attn.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.self_attn.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.self_attn.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.self_attn.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.self_attn.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.self_attn.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.self_attn.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.self_attn.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="c1"># Memory attention layers
</span>        <span class="sh">"</span><span class="s">memory_attention.layers.0.self_attn.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.self_attn.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.self_attn.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.self_attn.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.cross_attn_image.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.cross_attn_image.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.cross_attn_image.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.0.cross_attn_image.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.self_attn.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.self_attn.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.self_attn.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.self_attn.out_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.cross_attn_image.q_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.cross_attn_image.k_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.cross_attn_image.v_proj</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">memory_attention.layers.1.cross_attn_image.out_proj</span><span class="sh">"</span>
    <span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>The above code snippet is the LoRA configuration we chose to perform fine-tuning on SAMURAI using the CAMUS dataset. We chose the rank of the low-rank matrices \(r\) to be 8. Several mask decoder and memory attention layers are chosen to be fine-tuned to perform echocardiogram video segmentation due to the following reasons:</p> <ul> <li> <strong>Mask Decoder Focus</strong> <ul> <li>Direct Impact on Segmentation Quality: The mask decoder is the final component responsible for generating precise object masks.</li> <li>Task-Specific Refinement: Fine-tuning via LoRA here refines how the model attends to features for domain-specific or specialized segmentation tasks.</li> </ul> </li> <li> <strong>Memory Attention for Temporal Consistency</strong> <ul> <li>Frame-to-Frame Tracking: Memory attention layers store and reference information from prior frames, improving object continuity over time.</li> <li>Adaptation to Video Dynamics: LoRA-based updates to memory attention enable better handling of changes in object appearance, motion, or occlusions across frames.</li> </ul> </li> <li> <strong>Parameter-Efficient Fine-Tuning</strong> <ul> <li>Low-Rank Updates Only: LoRA imposes rank constraints on attention projections, reducing the number of trainable parameters.</li> <li>Preservation of Generality: Keeping the encoders frozen maintains broad, robust feature extraction while still allowing targeted improvements for video segmentation performance.</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hunter Ma 马思博. Last updated: January 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"cv",description:"You can check out my CV on the right which contains some of my essential research/work experiences and technical projects. Below is a more exhaustive list which provides more content.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-projects",title:"projects",description:"Here are some of my work and personal projects. This page is still being updated for more content, please stay tuned! Currently I finished the Deep Learning-Based Dynamical System Solver, CubeSat Control System, and Echocardiogram Segmentation pages. The images you see here are generated using AI.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"publications",description:"Currently I have 3 co-authored papers in progress. Two from Summer 2024 with CViSS at UWaterloo and one from Fall 2024 with RCL at UBC. My teammates and I are also thinking of turning our Deep Learning-based Dynamical System Solver report into a paper if time permits. I will update this page if more news comes through!",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"Here are some of the repositories that I have contributed before or I am currently working on. I try to keep myself busy with coding in the midst of semesters and internships. Note that the GitLab repo for the UBC Orbit Satellite Design Team has yet to be open-sourced, so unfortunately I am not able to share it here. Our team is working on open-sourcing part of it and it will hopefully be here in the near future.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-3d-gaussian-splatting",title:"3D Gaussian Splatting",description:"Computer Vision for Smart Structure Lab, UWaterloo",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-cubesat-control-system",title:"CubeSat Control System",description:"UBC Orbit Satellite Design Team, Attitude and Orbit Control System subteam",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-deep-learning-based-dynamical-system-solver",title:"Deep Learning-Based Dynamical System Solver",description:"UBC CPEN 355 Machine Learning self-directed course project",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-humam-vs-machine-air-hockey",title:"Humam vs Machine Air Hockey",description:"UBC Engineering Physics capstone project",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-autonomous-vehicle-competition",title:"Autonomous Vehicle Competition",description:"UBC ENPH 353 self-directed robotaxi challenge",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-echocardiogram-segmentation",title:"Echocardiogram Segmentation",description:"Robotics and Control Lab, UBC",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-rover-software-system",title:"Rover Software System",description:"UBC Rover Design Team, Software subteam",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-autonomous-robot-competition",title:"Autonomous Robot Competition",description:"UBC ENPH 253 self-directed robotics challenge, aka Engineering Physics Robot Summer",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%73%69%62%6F@%73%74%75%64%65%6E%74.%75%62%63.%63%61","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/HunterM321","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/huntermubc","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>